# Training Code Fixes - Summary

## ìˆ˜ì •ëœ ë¬¸ì œë“¤ ğŸ”§

### 1. âŒ â†’ âœ… Dataset ë¡œë”© (.json â†’ .jsonl)
**ë¬¸ì œ**: ì½”ë“œê°€ `.json` íŒŒì¼ì„ ê¸°ëŒ€í–ˆì§€ë§Œ ì‹¤ì œ ë°ì´í„°ëŠ” `.jsonl` í˜•ì‹
**ìˆ˜ì •**:
```python
# Before
with open(data_path) as f:
    self.data = json.load(f)

# After
self.data = []
with open(data_path) as f:
    for line in f:
        self.data.append(json.loads(line.strip()))
```

---

### 2. âŒ â†’ âœ… Label ìƒì„± ë¡œì§ (ê°€ì¥ ì¤‘ìš”!)
**ë¬¸ì œ**:
- ì›ë˜ ì½”ë“œëŠ” promptì˜ `input_ids`ë¥¼ ë³µì‚¬í•´ì„œ labelsë¥¼ ë§Œë“¦
- Target í† í°ì´ ì‹¤ì œë¡œ ì¶”ê°€ë˜ì§€ ì•ŠìŒ
- ëª¨ë¸ì´ ë°°ìš¸ ê²ƒì´ ì—†ì—ˆìŒ!

**ìˆ˜ì •**:
```python
# Before (WRONG)
labels = inputs["input_ids"].clone()  # Only prompt, no target!
prompt_length = inputs["input_ids"].size(1) - target_ids.size(1)  # Wrong math
labels[:, :prompt_length] = -100

# After (CORRECT)
# 1. Prompt tokenization
inputs = processor(images=image, text=prompt, return_tensors="pt")

# 2. Target tokenization (separately)
target_encoding = processor.tokenizer(
    target,
    add_special_tokens=False,
    return_tensors="pt",
)

# 3. Concatenate: [prompt_tokens, target_tokens, eos]
input_ids = torch.cat([
    inputs["input_ids"],
    target_encoding["input_ids"],
    torch.tensor([[processor.tokenizer.eos_token_id]])
], dim=1)

# 4. Create labels with prompt masked
labels = input_ids.clone()
prompt_length = inputs["input_ids"].size(1)
labels[:, :prompt_length] = -100  # Mask prompt
# Now labels contains actual target tokens!
```

**ì™œ ì¤‘ìš”í•œê°€**:
- Before: Model sees `[prompt_tokens] â†’ labels: [masked, masked, ...]`
  - **Nothing to learn!**
- After: Model sees `[prompt_tokens, target_tokens] â†’ labels: [masked, masked, ..., "water", eos]`
  - **Can learn to generate target!**

---

### 3. âŒ â†’ âœ… Data Collator ì¶”ê°€
**ë¬¸ì œ**: Trainerì— `data_collator` ì§€ì • ì•ˆë¨ â†’ default collator ì‚¬ìš©

**ìˆ˜ì •**:
```python
@dataclass
class VisionLanguageDataCollator:
    """Custom collator for VLM that handles pixel_values and text properly"""

    def __call__(self, features: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
        pixel_values = torch.stack([f["pixel_values"] for f in features])
        input_ids = torch.stack([f["input_ids"] for f in features])
        attention_mask = torch.stack([f["attention_mask"] for f in features])
        labels = torch.stack([f["labels"] for f in features])

        return {
            "pixel_values": pixel_values,
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels,
        }

# Trainerì— ì¶”ê°€
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=datasets["train"],
    eval_dataset=datasets["validation"],
    data_collator=VisionLanguageDataCollator(),  # â† ì¶”ê°€!
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],
)
```

---

### 4. âš ï¸ â†’ âœ… Memory-efficient Dataset Loading
**ë¬¸ì œ**: `Dataset.from_dict()` ì‚¬ìš© â†’ ëª¨ë“  ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë”©

**ìˆ˜ì •**:
```python
# Before
train_hf = Dataset.from_dict({
    k: [train_dataset[i][k] for i in range(len(train_dataset))]
    for k in train_dataset[0].keys()
})

# After (lazy loading)
def train_gen():
    for i in range(len(train_dataset)):
        yield train_dataset[i]

train_hf = Dataset.from_generator(
    train_gen,
    features=datasets.Features({...})
)
```

---

### 5. âœ… Import ì¶”ê°€
```python
import datasets  # features ì •ì˜ì— í•„ìš”
```

---

## í…ŒìŠ¤íŠ¸ ë°©ë²• ğŸ§ª

### 1. í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
```bash
cd fortuna_android/fine-tuning
python test_training.py
```

ì´ ìŠ¤í¬ë¦½íŠ¸ê°€ í™•ì¸í•˜ëŠ” ê²ƒ:
- âœ“ Dataset íŒŒì¼ë“¤ì´ ì¡´ì¬í•˜ê³  ì˜¬ë°”ë¥¸ í˜•ì‹ì¸ê°€?
- âœ“ í•„ìš”í•œ íŒ¨í‚¤ì§€ë“¤ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ê°€?
- âœ“ Processorê°€ ë¡œë”©ë˜ëŠ”ê°€?
- âœ“ Dataset classê°€ ì˜¬ë°”ë¥´ê²Œ ì‘ë™í•˜ëŠ”ê°€?
- âœ“ Data collatorê°€ batchë¥¼ ì˜¬ë°”ë¥´ê²Œ ë§Œë“œëŠ”ê°€?
- âœ“ **Labelsì— ì‹¤ì œ target í† í°ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€?**

### 2. ì‘ì€ í•™ìŠµ í…ŒìŠ¤íŠ¸
```bash
python finetune_smolvlm_v2.py \
    --dataset_dir ./dataset \
    --output_dir ./models/test_run \
    --num_epochs 1 \
    --batch_size 2 \
    --gradient_accumulation_steps 2 \
    --learning_rate 2e-4 \
    --bf16 \
    --wandb_project smolvlm-test
```

Lossê°€ ê°ì†Œí•˜ëŠ”ì§€ í™•ì¸!

---

## ìˆ˜ì • ì „ vs ìˆ˜ì • í›„ ë¹„êµ

### ìˆ˜ì • ì „ (ì‘ë™ ì•ˆí•¨)
```
1. Dataset: .json íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ âŒ
2. Labels: Target í† í° ì—†ìŒ âŒ
   â†’ Modelì´ ë°°ìš¸ ê²ƒì´ ì—†ìŒ
   â†’ Lossê°€ ê°ì†Œí•˜ì§€ ì•ŠìŒ
3. Collator: Default ì‚¬ìš© (VLMì— ìµœì í™” ì•ˆë¨) âš ï¸
4. Memory: ì „ì²´ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë”© âš ï¸
```

### ìˆ˜ì • í›„ (ì‘ë™í•¨)
```
1. Dataset: .jsonl ì˜¬ë°”ë¥´ê²Œ ë¡œë”© âœ…
2. Labels: Target í† í° í¬í•¨ âœ…
   â†’ Modelì´ "water", "fire" ë“±ì„ ìƒì„±í•˜ë„ë¡ í•™ìŠµ
   â†’ Lossê°€ ì •ìƒì ìœ¼ë¡œ ê°ì†Œ
3. Collator: VLMìš© ì»¤ìŠ¤í…€ collator âœ…
4. Memory: Generatorë¡œ lazy loading âœ…
```

---

## ì˜ˆìƒë˜ëŠ” í•™ìŠµ ê²°ê³¼

### ì •ìƒì ì¸ í•™ìŠµ ë¡œê·¸:
```
Epoch 1/3
Step 10: loss=2.456  (ë†’ìŒ - ì´ˆê¸°)
Step 20: loss=1.823  (ê°ì†Œ ì¤‘)
Step 30: loss=1.234  (ê³„ì† ê°ì†Œ)
...
Step 100: loss=0.456 (ë‚®ì•„ì§ - ì¢‹ìŒ!)

Validation: accuracy=0.72 (72% ì •í™•ë„)
```

### ë¬¸ì œê°€ ìˆì„ ë•Œ:
```
Epoch 1/3
Step 10: loss=2.456
Step 20: loss=2.451  (ê±°ì˜ ë³€í™” ì—†ìŒ - ë‚˜ì¨!)
Step 30: loss=2.449
...
Step 100: loss=2.401  (ê±°ì˜ ì•ˆ ë–¨ì–´ì§ - ë¬¸ì œ!)

Validation: accuracy=0.20 (20% - ëœë¤ë³´ë‹¤ ë‚˜ì¨)
```

---

## ë‹¤ìŒ ë‹¨ê³„

1. **í…ŒìŠ¤íŠ¸ ì‹¤í–‰**
   ```bash
   python test_training.py
   ```

2. **ì‘ì€ í•™ìŠµ í…ŒìŠ¤íŠ¸** (1 epoch, 2 batch)
   ```bash
   python finetune_smolvlm_v2.py \
       --dataset_dir ./dataset \
       --output_dir ./models/test_run \
       --num_epochs 1 \
       --batch_size 2
   ```

3. **Loss í™•ì¸**
   - Lossê°€ ê°ì†Œí•˜ëŠ”ê°€? âœ…
   - Accuracyê°€ ì˜¬ë¼ê°€ëŠ”ê°€? âœ…

4. **ì „ì²´ í•™ìŠµ**
   ```bash
   python finetune_smolvlm_v2.py \
       --dataset_dir ./dataset \
       --output_dir ./models/smolvlm-element \
       --num_epochs 3 \
       --batch_size 4 \
       --gradient_accumulation_steps 4 \
       --bf16
   ```

---

## ì£¼ìš” ê°œì„  ì‚¬í•­ ìš”ì•½

| í•­ëª© | ìˆ˜ì • ì „ | ìˆ˜ì • í›„ | ì¤‘ìš”ë„ |
|------|---------|---------|--------|
| Dataset ë¡œë”© | `.json` (ì‹¤íŒ¨) | `.jsonl` (ì„±ê³µ) | ğŸ”´ Critical |
| Label ìƒì„± | Target ì—†ìŒ | Target í¬í•¨ | ğŸ”´ Critical |
| Data Collator | Default | Custom VLM | ğŸŸ¡ Important |
| Memory íš¨ìœ¨ì„± | from_dict | from_generator | ğŸŸ¢ Nice-to-have |

**ê°€ì¥ ì¤‘ìš”í•œ ìˆ˜ì •**: Label ìƒì„± ë¡œì§!
- ì´ì „: ëª¨ë¸ì´ ë°°ìš¸ ë°ì´í„°ê°€ ì—†ì—ˆìŒ
- ì´ì œ: ëª¨ë¸ì´ "water", "fire" ë“±ì„ ì˜¬ë°”ë¥´ê²Œ ìƒì„±í•˜ë„ë¡ í•™ìŠµ ê°€ëŠ¥

---

## ë¬¸ì œê°€ ìƒê¸°ë©´?

### Q: "No module named 'torch'" ì—ëŸ¬
**A**: íŒ¨í‚¤ì§€ ì„¤ì¹˜ í•„ìš”
```bash
pip install -r requirements.txt
```

### Q: Lossê°€ ê°ì†Œí•˜ì§€ ì•ŠëŠ”ë‹¤
**A**: test_training.py ì‹¤í–‰í•´ì„œ labelsê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸
```bash
python test_training.py
# "Labels: X target tokens, Y masked (prompt)" ë¶€ë¶„ í™•ì¸
# target tokensê°€ 0ì´ë©´ ì•ˆë¨!
```

### Q: CUDA out of memory
**A**: Batch size ì¤„ì´ê¸°
```bash
python finetune_smolvlm_v2.py \
    --batch_size 2 \
    --gradient_accumulation_steps 8
```

### Q: "train.jsonl not found"
**A**: ê²½ë¡œ í™•ì¸
```bash
ls -la dataset/
# train.jsonlê³¼ val.jsonlì´ ìˆì–´ì•¼ í•¨
```

---

ë! ğŸ‰
