#!/usr/bin/env python3
"""
Test script to validate training code fixes
Run this before full training to catch issues early
"""

import sys
from pathlib import Path
import json

def test_dataset_loading():
    """Test that dataset files can be loaded"""
    print("=" * 80)
    print("TEST 1: Dataset Loading")
    print("=" * 80)

    dataset_dir = Path("dataset")

    # Check files exist
    train_file = dataset_dir / "train.jsonl"
    val_file = dataset_dir / "val.jsonl"

    if not train_file.exists():
        print(f"‚ùå FAIL: {train_file} not found")
        return False
    if not val_file.exists():
        print(f"‚ùå FAIL: {val_file} not found")
        return False

    # Load and check format
    with open(train_file) as f:
        first_line = f.readline()
        try:
            first_item = json.loads(first_line)
            print(f"‚úì Train file format: OK")
            print(f"  Keys: {list(first_item.keys())}")
            print(f"  Element: {first_item['element']}")
            print(f"  Image path: {first_item['image_path']}")
        except Exception as e:
            print(f"‚ùå FAIL: Could not parse train.jsonl: {e}")
            return False

    # Count samples
    with open(train_file) as f:
        train_count = sum(1 for _ in f)
    with open(val_file) as f:
        val_count = sum(1 for _ in f)

    print(f"‚úì Train samples: {train_count}")
    print(f"‚úì Val samples: {val_count}")

    # Check image exists
    image_path = dataset_dir / first_item['image_path']
    if not image_path.exists():
        print(f"‚ùå FAIL: Image not found: {image_path}")
        return False
    print(f"‚úì Sample image exists: {image_path}")

    print("‚úÖ PASS: Dataset loading\n")
    return True


def test_imports():
    """Test that all required packages can be imported"""
    print("=" * 80)
    print("TEST 2: Package Imports")
    print("=" * 80)

    packages = [
        ("torch", "PyTorch"),
        ("transformers", "Transformers"),
        ("peft", "PEFT"),
        ("datasets", "Datasets"),
        ("PIL", "Pillow"),
        ("wandb", "Weights & Biases"),
    ]

    all_ok = True
    for module, name in packages:
        try:
            __import__(module)
            print(f"‚úì {name}")
        except ImportError:
            print(f"‚ùå {name} not installed")
            all_ok = False

    if all_ok:
        print("‚úÖ PASS: All packages installed\n")
    else:
        print("‚ùå FAIL: Missing packages. Run: pip install -r requirements.txt\n")

    return all_ok


def test_processor():
    """Test processor loading"""
    print("=" * 80)
    print("TEST 3: Processor Loading")
    print("=" * 80)

    try:
        from transformers import AutoProcessor

        model_name = "HuggingFaceM4/SmolVLM2-500M-Video-Instruct"
        print(f"Loading processor from {model_name}...")

        processor = AutoProcessor.from_pretrained(model_name)
        print(f"‚úì Processor loaded")
        print(f"  Tokenizer vocab size: {len(processor.tokenizer)}")
        print(f"  Has image processor: {hasattr(processor, 'image_processor')}")

        # Test tokenization
        text = "water"
        tokens = processor.tokenizer(text, add_special_tokens=False)
        print(f"  'water' tokenizes to: {tokens['input_ids']}")

        print("‚úÖ PASS: Processor loading\n")
        return True

    except Exception as e:
        print(f"‚ùå FAIL: {e}\n")
        return False


def test_dataset_class():
    """Test AndroidAlignedDataset class (new messages format)"""
    print("=" * 80)
    print("TEST 4: Dataset Class")
    print("=" * 80)

    try:
        from finetune_smolvlm_v2 import AndroidAlignedDataset
        from transformers import AutoProcessor
        from pathlib import Path

        processor = AutoProcessor.from_pretrained("HuggingFaceM4/SmolVLM2-500M-Video-Instruct")

        dataset = AndroidAlignedDataset(
            data_path=Path("dataset/train.jsonl"),
            dataset_dir=Path("dataset"),
            processor=processor,
            max_length=128,
        )

        print(f"‚úì Dataset initialized with {len(dataset)} samples")

        # Get first item
        print("Loading first item...")
        item = dataset[0]

        print(f"‚úì Item loaded")
        print(f"  Keys: {list(item.keys())}")

        # Check new format
        if "messages" not in item:
            print("‚ùå FAIL: 'messages' key not found!")
            return False
        if "image" not in item:
            print("‚ùå FAIL: 'image' key not found!")
            return False

        print(f"  Image type: {type(item['image'])}")
        print(f"  Messages structure: {len(item['messages'])} messages")
        print(f"  User message content types: {[c['type'] for c in item['messages'][0]['content']]}")
        print(f"  Assistant response: {item['messages'][1]['content'][0]['text']}")

        # Try applying chat template
        try:
            text = processor.apply_chat_template(item['messages'], tokenize=False)
            print(f"  Chat template applied successfully")
            print(f"  Template output length: {len(text)} chars")
            print(f"  Contains target: {'fire' in text or 'water' in text or 'metal' in text or 'wood' in text or 'land' in text}")
        except Exception as e:
            print(f"‚ùå FAIL: Chat template failed: {e}")
            return False

        print("‚úÖ PASS: Dataset class works\n")
        return True

    except Exception as e:
        import traceback
        print(f"‚ùå FAIL: {e}")
        traceback.print_exc()
        print()
        return False


def test_collator():
    """Test data collator (new chat template approach)"""
    print("=" * 80)
    print("TEST 5: Data Collator")
    print("=" * 80)

    try:
        from finetune_smolvlm_v2 import VisionLanguageDataCollator, AndroidAlignedDataset
        from transformers import AutoProcessor
        from pathlib import Path

        processor = AutoProcessor.from_pretrained("HuggingFaceM4/SmolVLM2-500M-Video-Instruct")

        dataset = AndroidAlignedDataset(
            data_path=Path("dataset/train.jsonl"),
            dataset_dir=Path("dataset"),
            processor=processor,
            max_length=128,
        )

        collator = VisionLanguageDataCollator(processor=processor)

        # Create batch
        batch = [dataset[0], dataset[1]]
        print(f"  Sample 0 messages: {len(batch[0]['messages'])} messages")
        print(f"  Sample 1 messages: {len(batch[1]['messages'])} messages")

        collated = collator(batch)

        print(f"‚úì Batch collated")
        print(f"  Keys: {list(collated.keys())}")
        print(f"  pixel_values shape: {collated['pixel_values'].shape}")
        print(f"  input_ids shape: {collated['input_ids'].shape}")
        print(f"  attention_mask shape: {collated['attention_mask'].shape}")
        print(f"  labels shape: {collated['labels'].shape}")

        # Check batch size
        batch_size = collated['pixel_values'].shape[0]
        if batch_size != 2:
            print(f"‚ùå FAIL: Expected batch size 2, got {batch_size}")
            return False

        # CRITICAL: Check that labels contain actual tokens (not all -100)
        labels = collated['labels']
        num_non_masked = (labels != -100).sum().item()
        num_masked = (labels == -100).sum().item()
        total = labels.numel()

        print(f"  Label statistics:")
        print(f"    Total tokens: {total}")
        print(f"    Masked (-100): {num_masked} ({num_masked/total*100:.1f}%)")
        print(f"    Target tokens: {num_non_masked} ({num_non_masked/total*100:.1f}%)")

        if num_non_masked == 0:
            print("‚ùå FAIL: No target tokens in labels! Model has nothing to learn!")
            return False

        if num_non_masked < 4:  # Should have at least a few tokens per sample
            print(f"‚ö†Ô∏è  WARNING: Very few target tokens ({num_non_masked}). Expected more.")

        print("‚úÖ PASS: Data collator works\n")
        return True

    except Exception as e:
        import traceback
        print(f"‚ùå FAIL: {e}")
        traceback.print_exc()
        print()
        return False


def main():
    """Run all tests"""
    print("\n" + "=" * 80)
    print("SmolVLM Training Code Validation")
    print("=" * 80 + "\n")

    results = []

    # Run tests
    results.append(("Dataset Loading", test_dataset_loading()))
    results.append(("Package Imports", test_imports()))

    # Only run these if packages are installed
    if results[-1][1]:
        results.append(("Processor Loading", test_processor()))
        results.append(("Dataset Class", test_dataset_class()))
        results.append(("Data Collator", test_collator()))

    # Summary
    print("=" * 80)
    print("SUMMARY")
    print("=" * 80)

    for name, passed in results:
        status = "‚úÖ PASS" if passed else "‚ùå FAIL"
        print(f"{status}: {name}")

    all_passed = all(r[1] for r in results)

    print()
    if all_passed:
        print("üéâ All tests passed! Ready to train.")
        print()
        print("To start training, run:")
        print("  python finetune_smolvlm_v2.py \\")
        print("    --dataset_dir ./dataset \\")
        print("    --output_dir ./models/test_run \\")
        print("    --num_epochs 1 \\")
        print("    --batch_size 2 \\")
        print("    --bf16")
        return 0
    else:
        print("‚ùå Some tests failed. Fix the issues above before training.")
        return 1


if __name__ == "__main__":
    sys.exit(main())
