# Fine-tuning Pipeline - Code Structure Review

## ğŸ“ File Structure Overview

```
fine-tuning/
â”œâ”€â”€ ğŸ“– Documentation (4 files)
â”‚   â”œâ”€â”€ README.md                    # Main guide
â”‚   â”œâ”€â”€ REMOTE_SETUP.md             # L40S server setup
â”‚   â”œâ”€â”€ TRAINING_FLOW.md            # Visual flow diagrams
â”‚   â””â”€â”€ CHANGES_V2.md               # V2 changelog
â”‚
â”œâ”€â”€ ğŸ Dataset Preparation (3 files)
â”‚   â”œâ”€â”€ prepare_dataset.py          # [DEPRECATED] Unsplash version
â”‚   â”œâ”€â”€ prepare_dataset_coco.py     # [DEPRECATED] V2 COCO version
â”‚   â””â”€â”€ prepare_dataset_coco_v3.py  # âœ… USE THIS! Rich teacher prompts
â”‚
â”œâ”€â”€ ğŸ Training Scripts (2 files)
â”‚   â”œâ”€â”€ finetune_smolvlm.py         # [DEPRECATED] V1 (512x512)
â”‚   â””â”€â”€ finetune_smolvlm_v2.py      # âœ… USE THIS! Android-aligned
â”‚
â”œâ”€â”€ ğŸ Post-training (2 files)
â”‚   â”œâ”€â”€ merge_lora.py               # âœ… Merge LoRA into base model
â”‚   â””â”€â”€ validate_model.py           # âœ… Accuracy evaluation
â”‚
â”œâ”€â”€ ğŸ”§ Automation Scripts (3 files)
â”‚   â”œâ”€â”€ convert_to_gguf.sh          # âœ… FP16 â†’ Q8_0 quantization
â”‚   â”œâ”€â”€ quickstart.sh               # âœ… Local one-click pipeline
â”‚   â””â”€â”€ run_remote.sh               # âœ… Remote background pipeline
â”‚
â””â”€â”€ ğŸ“¦ Dependencies
    â””â”€â”€ requirements.txt             # âœ… Python packages
```

---

## âœ… RECOMMENDED Files (Use These!)

### 1. Dataset Preparation
**File**: `prepare_dataset_coco_v3.py`

**Why**:
- âœ… Rich teacher prompts (GPT-4V)
- âœ… Simple student prompts (SmolVLM)
- âœ… 256x256 Android-aligned images
- âœ… COCO dataset (free & fast)

**Key Features**:
```python
# Rich teacher prompt (500+ tokens)
teacher_prompt = """
Analyze visual features, colors, textures...
Provide detailed reasoning...
Color breakdown, alternative elements...
"""

# Simple student prompt (20 tokens)
student_prompt = "<__media__>\nClassify... Element:"

# Android-aligned preprocessing
image_256 = resize_for_android(image, 256)  # Black padding
```

**Usage**:
```bash
python prepare_dataset_coco_v3.py \
    --output_dir ./dataset \
    --num_images 1000 \
    --openai_api_key $OPENAI_API_KEY
```

---

### 2. Fine-tuning
**File**: `finetune_smolvlm_v2.py`

**Why**:
- âœ… 256x256 images (matches Android)
- âœ… `<__media__>` marker (matches Android)
- âœ… Max 32 tokens (matches Android)
- âœ… W&B tracking
- âœ… LoRA efficient training

**Key Features**:
```python
# Android alignment constants
ANDROID_IMAGE_SIZE = 256      # SmolVLMManager.kt:35
ANDROID_MAX_TOKENS = 32       # LLamaAndroid.kt:49
ANDROID_IMAGE_MARKER = "<__media__>"  # SmolVLMManager.kt:32

# Resize exactly like Android
def resize_for_android(image, 256):
    # Scale + black padding
    # Matches SmolVLMManager.kt:278-321
```

**Usage**:
```bash
python finetune_smolvlm_v2.py \
    --dataset_dir ./dataset \
    --output_dir ./models/smolvlm-element \
    --batch_size 4 \
    --num_epochs 3 \
    --bf16
```

---

### 3. Complete Pipeline
**File**: `run_remote.sh`

**Why**:
- âœ… Runs entire pipeline (dataset â†’ GGUF)
- âœ… Background execution with logging
- âœ… Uses V3 dataset + V2 training
- âœ… Error handling
- âœ… GPU monitoring

**Usage**:
```bash
# On L40S server
export OPENAI_API_KEY="sk-..."
export WANDB_API_KEY="..."

nohup ./run_remote.sh > logs/training.log 2>&1 &
tail -f logs/training.log
```

**Pipeline**:
```
1. Download COCO (one-time)         â†’ 5-10 min
2. Label with GPT-4V (V3)           â†’ 30-60 min
3. Fine-tune (V2)                   â†’ 4-8 hours
4. Merge LoRA                       â†’ 5 min
5. Convert to GGUF Q8_0            â†’ 10 min
6. Validate                         â†’ 10 min
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Total: 5-10 hours
```

---

## âŒ DEPRECATED Files (Don't Use!)

### 1. `prepare_dataset.py`
**Why deprecated**:
- âŒ Uses Unsplash (rate limited: 50 req/hr)
- âŒ Takes ~20 hours for 1000 images
- âŒ Requires Unsplash API key

**Replace with**: `prepare_dataset_coco_v3.py`

---

### 2. `prepare_dataset_coco.py`
**Why deprecated**:
- âŒ Simple teacher prompts (doesn't leverage GPT-4V fully)
- âŒ No rich context preservation

**Replace with**: `prepare_dataset_coco_v3.py`

---

### 3. `finetune_smolvlm.py`
**Why deprecated**:
- âŒ Trains on 512x512 images (Android uses 256x256)
- âŒ Uses `<image>` marker (Android uses `<__media__>`)
- âŒ Max 128 tokens (Android uses 32)
- âŒ Distribution shift between training/inference

**Replace with**: `finetune_smolvlm_v2.py`

---

## ğŸ” Code Quality Checks

### 1. Dataset Preparation V3
**File**: `prepare_dataset_coco_v3.py`

âœ… **Strengths**:
- Rich teacher prompts (300+ tokens)
- Android-aligned preprocessing (256x256, black padding)
- Proper error handling
- Progress bars
- Confidence filtering

âœ… **Code Quality**:
```python
def resize_for_android(image, target_size=256):
    """Matches SmolVLMManager.kt:278-321"""
    # Clear documentation
    # Type hints
    # Defensive checks
```

âš ï¸ **Potential Issues**:
```python
# Line ~295: Error handling could be more specific
except Exception as e:
    print(f"âŒ Error: {e}")
    return None

# Better:
except json.JSONDecodeError as e:
    print(f"âŒ JSON parse error: {e}")
except requests.RequestException as e:
    print(f"âŒ API error: {e}")
```

**Recommendation**: Good as-is, minor improvements optional

---

### 2. Fine-tuning V2
**File**: `finetune_smolvlm_v2.py`

âœ… **Strengths**:
- Android alignment constants
- Clear documentation
- W&B integration
- Type hints
- Proper data pipeline

âœ… **Code Quality**:
```python
@dataclass
class ModelConfig:
    """Clear configuration"""
    base_model: str = "..."
    lora_r: int = 16

ANDROID_IMAGE_SIZE = 256  # Well-documented constants
```

âš ï¸ **Potential Issues**:
```python
# Line ~145: Dataset conversion could be more efficient
# Currently loads all data into memory
train_hf = Dataset.from_dict({
    k: [train_dataset[i][k] for i in range(len(train_dataset))]
    for k in train_dataset[0].keys()
})

# Better: Use Dataset.from_generator for large datasets
def gen():
    for i in range(len(train_dataset)):
        yield train_dataset[i]

train_hf = Dataset.from_generator(gen)
```

**Recommendation**: Good as-is for 1000 images, optimize if scaling to 10k+

---

### 3. Run Remote Script
**File**: `run_remote.sh`

âœ… **Strengths**:
- Environment validation
- Error handling (`set -e`)
- Logging
- Progress tracking
- Cleanup

âœ… **Code Quality**:
```bash
# Good practices
set -e                    # Exit on error
set -o pipefail          # Catch pipe errors
mkdir -p logs            # Safe directory creation
TIMESTAMP=$(date +...)   # Unique log files
```

âš ï¸ **Potential Issues**:
```bash
# Line ~60: Could add timeout for API calls
# Currently no timeout if GPT-4V hangs

# Add to prepare_dataset_coco_v3.py:
response = client.chat.completions.create(
    timeout=60.0  # 60 second timeout
)
```

**Recommendation**: Add timeout in Python script

---

## ğŸ¯ Critical Path Analysis

### Minimum Required Files (L40S â†’ Android)

```
1. prepare_dataset_coco_v3.py     # Dataset
2. finetune_smolvlm_v2.py         # Training
3. merge_lora.py                  # Merge
4. convert_to_gguf.sh             # Quantize
5. requirements.txt               # Dependencies
```

**Or just use**:
```
run_remote.sh  # Calls all of above automatically
```

---

## ğŸ”§ Dependency Check

### requirements.txt
```python
torch>=2.1.0          # âœ… Core
transformers>=4.36.0  # âœ… Core
peft>=0.7.0          # âœ… LoRA
openai>=1.6.0        # âœ… GPT-4V labeling
wandb>=0.16.0        # âœ… Tracking
scikit-learn         # âœ… Validation
```

**Missing**: None - all essential packages included

**Recommendation**: âœ… Good!

---

## ğŸ“Š Integration Test

### Full Pipeline Test
```bash
# Should work end-to-end:
export OPENAI_API_KEY="sk-test"
export WANDB_API_KEY="test"

# Test dataset (10 images)
python prepare_dataset_coco_v3.py \
    --num_images 10 \
    --output_dir ./test_dataset

# Test training (1 epoch)
python finetune_smolvlm_v2.py \
    --dataset_dir ./test_dataset \
    --output_dir ./test_model \
    --num_epochs 1 \
    --batch_size 2

# Test merge
python merge_lora.py \
    --lora_path ./test_model/lora_adapter \
    --output_path ./test_model/merged

# Clean up
rm -rf test_dataset test_model
```

**Expected**: âœ… Should run without errors

---

## ğŸ› Known Issues & Workarounds

### Issue 1: OOM on Small GPUs
```bash
# Symptom: CUDA out of memory
# Fix: Reduce batch size
python finetune_smolvlm_v2.py --batch_size 2 --gradient_accumulation_steps 8
```

### Issue 2: GPT-4V Rate Limits
```python
# Symptom: 429 Too Many Requests
# Fix: Add sleep in prepare_dataset_coco_v3.py line ~295
time.sleep(1.0)  # Increase from 0.5
```

### Issue 3: GGUF Conversion Fails
```bash
# Symptom: llama.cpp not found
# Fix: Clone llama.cpp
cd ..
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp && make
```

---

## âœ… Final Checklist

### Before Running on L40S

- [ ] `export OPENAI_API_KEY="sk-..."`
- [ ] `export WANDB_API_KEY="..."`
- [ ] `chmod +x run_remote.sh`
- [ ] `pip install -r requirements.txt`
- [ ] llama.cpp cloned in `../llama.cpp`
- [ ] ~30GB disk space available
- [ ] GPU visible: `nvidia-smi`

### After Training

- [ ] Validation accuracy â‰¥85%
- [ ] GGUF files created (~521MB total)
- [ ] Model outputs single word labels
- [ ] Download to local machine
- [ ] Copy to Android assets
- [ ] Update config files
- [ ] Test on Android device

---

## ğŸ¯ Recommended Execution Path

### For Production (1000 images):
```bash
# Use run_remote.sh (handles everything)
./run_remote.sh
```

### For Testing (10 images):
```bash
# Manual steps for debugging
python prepare_dataset_coco_v3.py --num_images 10 ...
python finetune_smolvlm_v2.py --num_epochs 1 ...
```

### For Custom Datasets:
```bash
# Prepare your own dataset in same format
# Then use finetune_smolvlm_v2.py
```

---

## ğŸ“ˆ Performance Expectations

### Dataset Preparation (1000 images)
```
COCO download:     5-10 min (one-time)
GPT-4V labeling:   30-60 min
Total:             35-70 min
Cost:              ~$10-15 (GPT-4V API)
```

### Training (L40S)
```
Epochs:            3
Batch size:        4 (effective 16 with grad accum)
Duration:          4-8 hours
GPU memory:        ~30GB
Expected accuracy: 85-90%
```

### Quantization
```
Merge LoRA:        5 min
Convert to GGUF:   10 min
File size:         ~521MB (model + mmproj)
```

---

## ğŸ”’ Security Notes

### API Keys
```bash
# Never commit to git!
echo "*.env" >> .gitignore
echo "logs/*.log" >> .gitignore

# Use environment variables
export OPENAI_API_KEY="..."  # Not in code!
```

### Model Files
```bash
# .gitignore includes:
models/
dataset/
*.gguf
*.log
```

**Recommendation**: âœ… Already properly configured

---

## ğŸ“ Code Review Summary

| Component | File | Status | Notes |
|-----------|------|--------|-------|
| **Dataset Prep** | `prepare_dataset_coco_v3.py` | âœ… GOOD | Rich teacher prompts, Android-aligned |
| **Training** | `finetune_smolvlm_v2.py` | âœ… GOOD | Perfect Android alignment |
| **Merge** | `merge_lora.py` | âœ… GOOD | Simple, works well |
| **Convert** | `convert_to_gguf.sh` | âœ… GOOD | Proper error handling |
| **Validate** | `validate_model.py` | âœ… GOOD | Comprehensive metrics |
| **Pipeline** | `run_remote.sh` | âœ… GOOD | End-to-end automation |
| **Docs** | `*.md` | âœ… EXCELLENT | Clear, comprehensive |

### Overall Assessment: âœ… **PRODUCTION READY**

**Recommendations**:
1. Minor: Add timeout to GPT-4V calls
2. Minor: More specific exception handling
3. Optional: Batch dataset loading for 10k+ images

**Blockers**: None

**Ready for L40S deployment**: âœ… YES

---

## ğŸš€ Quick Start Command

```bash
# One-liner for L40S:
export OPENAI_API_KEY="sk-..." && \
export WANDB_API_KEY="..." && \
nohup ./run_remote.sh > logs/training_$(date +%Y%m%d_%H%M%S).log 2>&1 &

# Monitor:
tail -f logs/training_*.log
```

That's it! ğŸ‰
