# SmolVLM Training Guide - COMPLETE SETUP

## ‚úÖ What's Been Created

You now have a complete training pipeline for SmolVLM element classification:

### Core Scripts
- **`train_smolvlm.py`** - Main training script with LoRA fine-tuning
- **`validate.py`** - Validation and metrics evaluation
- **`check_dataset.py`** - Dataset quality analysis
- **`quickstart.sh`** - Automated setup and test script

### Configuration
- **`configs/mac.yaml`** - Mac M1/M2 (testing)
- **`configs/l40s.yaml`** - L40S server with 16GB RAM
- **`configs/colab.yaml`** - Google Colab Pro

### Documentation
- **`README.md`** - Complete usage guide
- **`requirements.txt`** - Python dependencies

## üöÄ Quick Start (Choose Your Platform)

### Option 1: Mac M1/M2 (For Testing Only)

```bash
# Install dependencies
pip3 install -r requirements.txt

# Quick test (1 sample to verify setup)
python3 train_smolvlm.py \
    --config configs/mac.yaml \
    --dataset_dir ./dataset \
    --output_dir ./output_test

# Full training (will take 2-3 hours)
python3 train_smolvlm.py \
    --config configs/mac.yaml \
    --dataset_dir ./dataset \
    --output_dir ./output
```

### Option 2: L40S Server (Recommended for Production)

```bash
# SSH to server, navigate to fortuna_ai directory

# Install dependencies
pip3 install -r requirements.txt

# Train
python3 train_smolvlm.py \
    --config configs/l40s.yaml \
    --dataset_dir ./dataset \
    --output_dir ./output

# Expected time: ~30-45 minutes
```

### Option 3: Google Colab Pro

```python
# In Colab notebook:

# Mount Drive and clone repo
from google.colab import drive
drive.mount('/content/drive')

!git clone https://github.com/YOUR_REPO fortuna
%cd fortuna/fortuna_ai

# Install dependencies
!pip install -r requirements.txt

# Train with W&B logging
!python train_smolvlm.py \
    --config configs/colab.yaml \
    --dataset_dir ./dataset \
    --output_dir ./output \
    --wandb

# Expected time: ~20-40 minutes (T4/A100)
```

## ‚ö†Ô∏è CRITICAL: Dataset Imbalance Issue

Your dataset has **severe class imbalance**:

```
Element Distribution (Training Set):
  metal: 344 samples (38%)  ‚Üê Overrepresented
  land:  233 samples (26%)
  wood:  175 samples (19%)
  water: 140 samples (16%)
  fire:    8 samples (1%)   ‚Üê SEVERELY underrepresented!
```

**Impact:**
- Model will **almost never** predict "fire"
- Fire detection accuracy will be **very poor** (<10%)
- Overall accuracy will be misleading

**Solutions** (in order of preference):

### 1. Collect More Fire Samples (BEST)
Add at least 100-150 fire images to balance the dataset:
- Flames, campfires, candles
- Sunlight, bright lights
- Anything with intense light/energy

### 2. Use Class Weights (Quick Fix)
The training script can handle imbalance, but fire will still be underrepresented.

### 3. Oversample Fire During Training
Duplicate fire samples to balance the classes.

### 4. Accept Lower Fire Accuracy
Train as-is and accept that fire detection will be poor. Not recommended for production.

## üìä Validation

After training, validate the model:

```bash
# Full validation
python3 validate.py \
    --model_dir ./output/final \
    --dataset_dir ./dataset

# Quick test (10 samples)
python3 validate.py \
    --model_dir ./output/final \
    --dataset_dir ./dataset \
    --max_samples 10

# Save results to JSON
python3 validate.py \
    --model_dir ./output/final \
    --dataset_dir ./dataset \
    --output results.json
```

**Expected Results:**
- Overall accuracy: 70-85% (limited by fire class)
- Water: ~80%
- Land: ~75%
- Fire: ~10-20% ‚ö†Ô∏è (due to severe underrepresentation)
- Wood: ~80%
- Metal: ~85%

## üîÑ Next Steps: Android Deployment

### 1. Merge LoRA Adapters

Create a Python script `merge_lora.py`:

```python
#!/usr/bin/env python3
from transformers import AutoModelForImageTextToText, AutoProcessor
from peft import PeftModel

print("Loading base model...")
base_model = AutoModelForImageTextToText.from_pretrained(
    "HuggingFaceTB/SmolVLM2-500M-Video-Instruct",
    trust_remote_code=True
)

print("Loading LoRA adapters...")
model = PeftModel.from_pretrained(base_model, "./output/final")

print("Merging...")
merged_model = model.merge_and_unload()

print("Saving merged model...")
merged_model.save_pretrained("./output/merged")

processor = AutoProcessor.from_pretrained("./output/final")
processor.save_pretrained("./output/merged")

print("‚úÖ Done! Merged model saved to ./output/merged")
```

Run it:
```bash
python3 merge_lora.py
```

### 2. Convert to GGUF

**Option A: Use llama.cpp (Recommended)**

```bash
# Clone llama.cpp if you haven't
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

# Install dependencies
pip install -r requirements.txt

# Convert to GGUF with Q8_0 quantization
python convert-hf-to-gguf.py \
    ../fortuna_ai/output/merged \
    --outfile ../fortuna_ai/output/SmolVLM2-500M-Element-Q8_0.gguf \
    --outtype q8_0

# Note: The mmproj conversion might need special handling
# Check llama.cpp docs for vision models
```

**Option B: Use Pre-converted Base GGUF**

Since you found the official GGUF at https://huggingface.co/ggml-org/SmolVLM2-500M-Video-Instruct-GGUF:

1. The base GGUF is for the **untuned** model
2. After fine-tuning, you still need to convert YOUR merged model
3. However, you can use the same conversion process as the official GGUF

### 3. Deploy to Android

1. Copy GGUF files to Android:
```bash
cp output/SmolVLM2-500M-Element-Q8_0.gguf \
   ../fortuna_android/app/src/main/assets/models/

# If separate mmproj needed:
cp output/mmproj-SmolVLM2-500M-Element-Q8_0.gguf \
   ../fortuna_android/app/src/main/assets/models/
```

2. Update `SmolVLMManager.kt`:
```kotlin
private const val MODEL_FILENAME = "SmolVLM2-500M-Element-Q8_0.gguf"
private const val MMPROJ_FILENAME = "mmproj-SmolVLM2-500M-Element-Q8_0.gguf"
```

3. Update prompt in `ARRenderer.kt` or create new constant:
```kotlin
// In classification/utils/Constants.kt
const val ELEMENT_CLASSIFICATION_PROMPT = """Classify this image into one of these elements: water, land, fire, wood, metal.

Element:"""
```

4. Modify `ARRenderer.kt` to:
   - Use object detection bounding boxes
   - Crop image to bounding box
   - Feed cropped region to SmolVLM
   - Parse element from output

## üêõ Troubleshooting

### Training Issues

**Out of Memory:**
- Reduce batch_size in config
- Increase gradient_accumulation_steps
- Enable gradient_checkpointing
- Use bf16 if on CUDA

**Training is slow:**
- Check you're using GPU (CUDA/MPS)
- Reduce max_length in config
- Reduce num_workers if on Mac

**Low accuracy:**
- Check dataset quality with `check_dataset.py`
- Train for more epochs
- Increase LoRA rank
- **Address fire class imbalance!**

### Validation Issues

**Model outputs wrong format:**
- Check training converged (loss should decrease)
- Increase max_new_tokens during validation
- Verify prompt format matches training

**Fire accuracy is 0%:**
- Expected due to only 8 fire samples in training
- Need to collect more fire data

### Deployment Issues

**GGUF conversion fails:**
- Check llama.cpp version compatibility
- Try different quantization levels (Q4_0, Q5_0, Q8_0)
- Check logs for specific errors

**Android model doesn't work:**
- Verify GGUF file size is reasonable (~500MB for Q8_0)
- Check Android logs for loading errors
- Test with base model first, then fine-tuned

## üìÅ File Structure After Training

```
fortuna_ai/
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îú‚îÄ‚îÄ mac.yaml
‚îÇ   ‚îú‚îÄ‚îÄ l40s.yaml
‚îÇ   ‚îî‚îÄ‚îÄ colab.yaml
‚îú‚îÄ‚îÄ dataset/
‚îÇ   ‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îú‚îÄ‚îÄ train.jsonl (900 samples)
‚îÇ   ‚îî‚îÄ‚îÄ val.jsonl (100 samples)
‚îú‚îÄ‚îÄ output/
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint-100/
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint-200/
‚îÇ   ‚îú‚îÄ‚îÄ final/               # Best model (LoRA adapters)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ adapter_config.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ adapter_model.safetensors
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ merged/              # Merged model (after merge_lora.py)
‚îÇ       ‚îú‚îÄ‚îÄ model.safetensors
‚îÇ       ‚îú‚îÄ‚îÄ config.json
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ train_smolvlm.py
‚îú‚îÄ‚îÄ validate.py
‚îú‚îÄ‚îÄ check_dataset.py
‚îú‚îÄ‚îÄ merge_lora.py            # Create this for merging
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

## ‚è±Ô∏è Time Estimates

| Task | Mac M1 | L40S (16GB) | Colab Pro (A100) |
|------|--------|-------------|------------------|
| Setup | 5 min | 2 min | 3 min |
| Training (2-3 epochs) | 2-3 hours | 30-45 min | 20-30 min |
| Validation | 5-10 min | 2-3 min | 1-2 min |
| Merge + Convert | 10 min | 5 min | 5 min |
| **Total** | **3-4 hours** | **45-60 min** | **30-40 min** |

## üéØ Success Criteria

- [x] Training completes without errors
- [x] Training loss decreases over epochs
- [x] Validation accuracy > 70% overall
- [x] Per-element accuracy > 60% (except fire due to imbalance)
- [x] Model generates correct format output (element names)
- [x] GGUF conversion succeeds
- [x] Android integration works

## üìû Getting Help

If you encounter issues:

1. Check this guide and README.md
2. Run `python check_dataset.py` to verify data
3. Check GPU/memory usage during training
4. Review training logs for errors
5. Test with smaller dataset first (--max_samples 10)

Good luck with training! üöÄ
